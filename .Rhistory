library(ggplot2)
library(gridExtra)
library(purrr)
library(car)
load("clean_data.RData")
# Extract relevant columns from clean_data
cog_df <- clean_data[, c("group","z_pvt_reaction_time_w","z_tmt_a_time_w","z_tmt_b_time_w")]
# Check structure and summarize contents of cog_df
str(cog_df)
summary(cog_df)
# Store group labels in a separate variable and exclude label (group column) from the dataset to do clustering
# Later true labels will be used to check how good clustering turned out
cog_label <- cog_df$group
cog_df$group <- NULL
str(cog_df)
# Build distance matrix
# Since all values are continuous numerical values I use euclidean distance method
dist_mat <- dist(cog_df, method = 'euclidean')
# Now decide which linkage method to use
# Try different kinds of linkage methods after decide which performed better
# Build dendrogram by plotting hierarchical cluster object with hclust
# Specify linkage method via 'method' argument
hclust_ward <- hclust(dist_mat, method = 'ward')
plot(hclust_ward)
# Create the desired number of clusters
# Since I want two groups 'withPCS' and 'withoutPCS' number of clusters = 2
cut_ward <- cutree(hclust_ward, k = 4)
# To visualize clusters on dendrogram use abline function to draw the cut line
plot(hclust_ward)
rect.hclust(hclust_ward, k = 4, border = 2:30)
abline(h = 10, col = 'red')
# Visualize tree with different colored branches
ward_dend_obj <- as.dendrogram(hclust_ward)
ward_col_dend <- color_branches(ward_dend_obj, h = 10)
plot(ward_col_dend)
# Append cluster results obtained back in the original dataframe
# Use mutate
# Count how many observations were assigned to each cluster with the count function
cog_df_cl <- mutate(cog_df, cluster = cut_ward)
count(cog_df_cl,cluster)
# Cross-checking clustering results using table function
table(cog_df_cl$cluster,cog_label)
# Add the cluster information from cog_df_cl to clean_data
clean_data$cluster <- cog_df_cl$cluster
# Check the updated structure of clean_data
str(clean_data)
# Checking sex in cluster
table(cog_df_cl$cluster,clean_data$sex)
cluster_info <- clean_data %>%
select(participant_id, cluster)
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
merged_data <- merged_data %>%
rename(cluster_4 = cluster)
merged_data <- merged_data %>%
mutate(cluster_4 = recode(cluster_4, '1' = 'c1', '2' = 'c2', '3' = 'c3'. '4' = 'c4'))
merged_data <- merged_data %>%
mutate(cluster_4 = recode(cluster_4, '1' = 'c1', '2' = 'c2', '3' = 'c3', '4' = 'c4'))
#--------
cluster_info <- clean_data %>%
select(participant_id, cluster)
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
merged_data <- merged_data %>%
rename(cluster_4 = cluster)
merged_data <- merged_data %>%
mutate(cluster_4 = recode(cluster_4, '1' = 'c1', '2' = 'c2', '3' = 'c3', '4' = 'c4'))
# Check the structure of the merged data to ensure everything is correct
str(merged_data)
# Optionally, save the merged data to a new file
write_tsv(merged_data, "C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data_all.tsv")
View(merged_data)
merged_data <- merged_data %>%
mutate(cluster_4 = recode(cluster_4, '1' = 'c1', '2' = 'c2', '3' = 'c3', '4' = 'c4'))
merged_data <- merged_data %>%
mutate(cluster_4 = as.character(cluster_4)) %>%
mutate(cluster_4 = recode(cluster_4, '1' = 'c1', '2' = 'c2', '3' = 'c3', '4' = 'c4'))
merged_data <- merged_data %>%
#mutate(cluster_4 = as.character(cluster_4)) %>%
mutate(cluster_4 = recode(cluster_4, `1` = 'c1', `2` = 'c2', `3` = 'c3', `4` = 'c4'))
merged_data <- merged_data %>%
mutate(cluster_4 = as.character(cluster_4)) %>%
mutate(cluster_4 = recode(cluster_4, `1` = 'c1', `2` = 'c2', `3` = 'c3', `4` = 'c4'))
#--------
cluster_info <- clean_data %>%
select(participant_id, cluster)
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
merged_data <- merged_data %>%
rename(cluster_4 = cluster)
# Convert cluster_4 to character if necessary
merged_data <- merged_data %>%
mutate(cluster_4 = as.character(cluster_4))
merged_data <- merged_data %>%
mutate(cluster_4 = recode(cluster_4, '1' = 'c1', '2' = 'c2', '3' = 'c3', '4' = 'c4'))
# Check the structure of the merged data to ensure everything is correct
str(merged_data)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
merged_data <- merged_data %>%
rename(cluster_4 = cluster)
str(merged_data)
# Convert cluster_4 to character if necessary
merged_data <- merged_data %>%
mutate(cluster_4 = as.character(cluster_4))
merged_data <- merged_data %>%
mutate(cluster_4 = recode(cluster_4, '1' = 'c1', '2' = 'c2', '3' = 'c3', '4' = 'c4'))
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
merged_data <- merged_data %>%
rename(cluster_4 = cluster)
str(merged_data)
# Convert cluster_4 to character if necessary
merged_data <- merged_data %>%
mutate(cluster_4 = as.character(cluster_4))
merged_data <- merged_data %>%
mutate(cluster_4 = recode(cluster_4, `1` = 'c1', `2` = 'c2', `3` = 'c3', `4` = 'c4'))
# Use dplyr::recode explicitly
merged_data <- merged_data %>%
mutate(cluster_4 = dplyr::recode(cluster_4, `1` = 'c1', `2` = 'c2', `3` = 'c3', `4` = 'c4'))
# Check the structure of the merged data to ensure everything is correct
str(merged_data)
# Optionally, save the merged data to a new file
write_tsv(merged_data, "C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data_all.tsv")
View(merged_data)
# Hierarchical clustering
library(tidyverse)
library(dplyr)
library(dendextend)
library(ggplot2)
library(gridExtra)
library(purrr)
library(vroom)
load("clean_data.RData")
# Extract relevant columns from clean_data
cog_df <- clean_data[, c("group","z_pvt_reaction_time_w","z_tmt_a_time_w","z_tmt_b_time_w")]
# Check structure and summarize contents of cog_df
str(cog_df)
summary(cog_df)
# Store group labels in a separate variable and exclude label (group column) from the dataset to do clustering
# Later true labels will be used to check how good clustering turned out
cog_label <- cog_df$group
cog_df$group <- NULL
str(cog_df)
# Build distance matrix
# Since all values are continuous numerical values I use euclidean distance method
dist_mat <- dist(cog_df, method = 'euclidean')
# Now decide which linkage method to use
# Try different kinds of linkage methods after decide which performed better
# Build dendrogram by plotting hierarchical cluster object with hclust
# Specify linkage method via 'method' argument
hclust_ward <- hclust(dist_mat, method = 'ward')
plot(hclust_ward)
# Create the desired number of clusters
# Since I want two groups 'withPCS' and 'withoutPCS' number of clusters = 2
cut_ward <- cutree(hclust_ward, k = 2)
# To visualize clusters on dendrogram use abline function to draw the cut line
plot(hclust_ward)
rect.hclust(hclust_ward, k = 2, border = 2:30)
abline(h = 28, col = 'red')
# Visualize tree with different colored branches
ward_dend_obj <- as.dendrogram(hclust_ward)
ward_col_dend <- color_branches(ward_dend_obj, h = 28)
plot(ward_col_dend)
# Visualize the clusters see YT Video Hierarchical Clustering in R Spencer Pao
# install.packages("factoextra")
# library(factoextra)
# cluster_obj <- list(data = cog_df_sc, cluster = cut_avg)
# fviz_cluster(cluster_obj)
# rownames(cog_df_sc) <- paste(cog_label, 1:dim(cog_df) [1], sep = "_")
# fviz_cluster(list(data=cog_df_sc))
# Append cluster results obtained back in the original dataframe
# Use mutate
# Count how many observations were assigned to each cluster with the count function
cog_df_cl <- mutate(cog_df, cluster = cut_ward)
count(cog_df_cl,cluster)
# Cross-checking clustering results using table function
table(cog_df_cl$cluster,cog_label)
# Add the cluster information from cog_df_cl to clean_data
clean_data$cluster <- cog_df_cl$cluster
# Check the updated structure of clean_data
str(clean_data)
# Checking sex in cluster
table(cog_df_cl$cluster,clean_data$sex)
cluster_info <- clean_data %>%
select(participant_id, cluster)
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/participants.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
merged_data <- merged_data %>%
rename(cluster_2 = cluster)
merged_data <- merged_data %>%
mutate(cluster_2 = as.character(cluster_2)) %>%
mutate(cluster_2 = recode(cluster_2, '1' = 'c1', '2' = 'c2'))
merged_data <- merged_data %>%
mutate(cluster_2 = recode(cluster_2, '1' = 'c1', '2' = 'c2'))
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
# Convert cluster_4 to character if necessary
merged_data <- merged_data %>%
mutate(cluster_2 = as.character(cluster_2))
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/participants.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
merged_data <- merged_data %>%
rename(cluster_2 = cluster)
# Convert cluster_4 to character if necessary
merged_data <- merged_data %>%
mutate(cluster_2 = as.character(cluster_2))
# Use dplyr::recode explicitly
merged_data <- merged_data %>%
mutate(cluster_2 = dplyr::recode(cluster_4, `1` = 'c1', `2` = 'c2'))
# Use dplyr::recode explicitly
merged_data <- merged_data %>%
mutate(cluster_2 = dplyr::recode(cluster_2, `1` = 'c1', `2` = 'c2'))
# Check the structure of the merged data to ensure everything is correct
str(merged_data)
# Optionally, save the merged data to a new file
write_tsv(merged_data, "C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data.tsv")
# Hierarchical clustering
library(tidyverse)
library(dplyr)
library(dendextend)
library(ggplot2)
library(gridExtra)
library(purrr)
library(car)
load("clean_data.RData")
# Extract relevant columns from clean_data
cog_df <- clean_data[, c("group","z_pvt_reaction_time_w","z_tmt_a_time_w","z_tmt_b_time_w")]
# Check structure and summarize contents of cog_df
str(cog_df)
summary(cog_df)
# Store group labels in a separate variable and exclude label (group column) from the dataset to do clustering
# Later true labels will be used to check how good clustering turned out
cog_label <- cog_df$group
cog_df$group <- NULL
str(cog_df)
# Build distance matrix
# Since all values are continuous numerical values I use euclidean distance method
dist_mat <- dist(cog_df, method = 'euclidean')
# Now decide which linkage method to use
# Try different kinds of linkage methods after decide which performed better
# Build dendrogram by plotting hierarchical cluster object with hclust
# Specify linkage method via 'method' argument
hclust_ward <- hclust(dist_mat, method = 'ward')
plot(hclust_ward)
# Create the desired number of clusters
# Since I want two groups 'withPCS' and 'withoutPCS' number of clusters = 2
cut_ward <- cutree(hclust_ward, k = 4)
# To visualize clusters on dendrogram use abline function to draw the cut line
plot(hclust_ward)
rect.hclust(hclust_ward, k = 4, border = 2:30)
abline(h = 10, col = 'red')
# Visualize tree with different colored branches
ward_dend_obj <- as.dendrogram(hclust_ward)
ward_col_dend <- color_branches(ward_dend_obj, h = 10)
plot(ward_col_dend)
# Append cluster results obtained back in the original dataframe
# Use mutate
# Count how many observations were assigned to each cluster with the count function
cog_df_cl <- mutate(cog_df, cluster = cut_ward)
count(cog_df_cl,cluster)
# Cross-checking clustering results using table function
table(cog_df_cl$cluster,cog_label)
# Add the cluster information from cog_df_cl to clean_data
clean_data$cluster <- cog_df_cl$cluster
# Check the updated structure of clean_data
str(clean_data)
# Checking sex in cluster
table(cog_df_cl$cluster,clean_data$sex)
#--------
cluster_info <- clean_data %>%
select(participant_id, cluster)
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
merged_data <- merged_data %>%
rename(cluster_4 = cluster)
str(merged_data)
# Convert cluster_4 to character if necessary
merged_data <- merged_data %>%
mutate(cluster_4 = as.character(cluster_4))
# Use dplyr::recode explicitly
merged_data <- merged_data %>%
mutate(cluster_4 = dplyr::recode(cluster_4, `1` = 'c1', `2` = 'c2', `3` = 'c3', `4` = 'c4'))
# Check the structure of the merged data to ensure everything is correct
str(merged_data)
# Optionally, save the merged data to a new file
write_tsv(merged_data, "C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data_all.tsv")
setwd("C:/Users/jankj/OneDrive/Desktop/masters_thesis")
#------------ 1. load packages------------------
library(tidyverse)
library(carData)
library(car)
library(readr)
library(ggdist)
library(ggExtra)# displaying distributions next to plots
library(ggsignif)# displaying stats in plots
library(ggplot2)
library(ggpubr)
library(coin)# need this for z value of wilcox test
library(effsize)# for cohens d
library(rstatix)# for wilcox test
table_power_5 <- read_csv("data/analysis_power/table_power_final_5.csv")
number_of_epochs_5 <- read_csv("data/analysis_power/number_of_epochs_5.csv")
# this would be the amount of people where more than half of the data is good
number_of_epochs%>%
filter(number_epochs > 37.5)# here 70
number_of_epochs%>%
filter(number_epochs > 50)# here 61
number_of_epochs%>%
filter(number_epochs < 51)# these subjects have to be excluded
number_of_epochs_5%>%
filter(number_epochs > 37.5)# here 62
number_of_epochs_5%>%
filter(number_epochs < 37.5)# these subjects have to be excluded
# modify table (f.ex. add tmt b-a)
table_power_5 <- table_power_5%>%
mutate(facit_f_FS = as.numeric(facit_f_FS),
tmt_b_minus_a = tmt_b_time-tmt_a_time)
table_power_5 <- merge(table_power_5, number_of_epochs_5)
test_table <- table_power_5%>%
group_by(participant_id,group,tmt_a_time,facit_f_FS, tmt_b_minus_a,age,years_of_education,number_epochs,cluster_2)%>%
summarise(mean_delta_power = mean(rel_delta),
mean_beta_power = mean(rel_beta),
mean_aperiodic_exponent = mean(aperiodic_exponent))
test_table%>%
group_by(cluster_2)%>%
summarise(mean_age = mean(age),
sd_age = sd(age),
mean_epochs = mean(number_epochs))# sind immerhin fast identisch vom Alter her
t.test(age~cluster_2, data = test_table, alternative = "two.sided")
# Define the channel names you want to select (for delta)
frontal_channels <- c('22','105','11','40','75','39','49','82','48','19','112','25','94','93','83','92','95','96','21','50','10','59','26')
# Filter rows with the specified channel names
table_power_frontal <- table_power_5%>%
filter(table_power_5$channel %in% frontal_channels)
# Define the channel names you want to select (for beta)
central_channels <- c('85','34','65','37','90','66','1','68','3','67','2','70','74','76','81','34','37','42','86','43','87','44','88','45','89','46','77','5','78','6','7','79','8','80','71','35','72','36','73')
# Filter rows with the specified channel names
table_power_central <- table_power_5%>%
filter(table_power_5$channel %in% central_channels)
#-------3. summarise mean -----------------
df_corr_frontal <- table_power_frontal%>%
group_by(participant_id,group,tmt_a_time,facit_f_FS, tmt_b_minus_a,age,years_of_education,sex,hads_d_total_score, number_epochs, cluster_2)%>%
summarise(mean_delta_power = mean(rel_delta),
mean_beta_power = mean(rel_beta),
mean_aperiodic_exponent = mean(aperiodic_exponent))
df_corr_central <- table_power_central%>%
group_by(participant_id,group,tmt_a_time,facit_f_FS, tmt_b_minus_a,hads_d_total_score, number_epochs, cluster_2)%>%
summarise(mean_delta_power = mean(rel_delta),
mean_beta_power = mean(rel_beta),
mean_aperiodic_exponent = mean(aperiodic_exponent))
#------- 4. demographics-----------------
# age
df_corr_frontal%>%
group_by(participant_id)%>%
ggplot(aes(age))+
geom_histogram(color = "black",
fill = "white", bins = sqrt(100))+
facet_wrap(~cluster_2,scales = 'free')+
theme_classic()
# years of education
df_corr_frontal%>%
group_by(participant_id)%>%
ggplot(aes(years_of_education))+
geom_histogram(color = "black",
fill = "white", bins = sqrt(100))+
facet_wrap(~cluster_2,scales = 'free')+
theme_classic()
t.test(years_of_education~cluster_2, data = df_corr_frontal, alternative = "two.sided")
wilcox.test(years_of_education~cluster_2, data = df_corr_frontal,
exact = FALSE,
correct = FALSE,
conf.int = FALSE)# without PCS hat eine schiefe Verteilung
df_corr_frontal%>%
group_by(cluster_2)%>%
count()
df_corr_frontal%>%
group_by(cluster_2,sex)%>%
count()
df_corr_frontal%>%
group_by(cluster_2)%>%
summarise(mean_age = mean(age),
sd_age = sd(age))
t.test(age~cluster_2, data = df_corr_frontal, alternative = "two.sided")
wilcox.test(age~cluster_2, data = df_corr_frontal,
exact = FALSE,
correct = FALSE,
conf.int = FALSE)# 0.69
table_behav <- df_corr_frontal%>%
group_by(cluster_2)%>%
summarise(mean_facit = mean(facit_f_FS, na.rm = T),
sd_facit = sd(facit_f_FS, na.rm = T),
mean_hads = mean(hads_d_total_score, na.rm = T),
sd_hads = sd(hads_d_total_score, na.rm = T),
mean_tmta = mean(tmt_a_time),
sd_tmta = sd(tmt_a_time),
mean_tmtb_a = mean(tmt_b_minus_a),
sd_tmtb_a = sd(tmt_b_minus_a),
mean_y_o = mean(years_of_education),
sd_y_o = sd(years_of_education),
mean_epoc = mean(number_epochs),
sd_epoc = sd(number_epochs))
df_corr_frontal%>%
group_by(cluster_2)%>%
ggplot(aes(facit_f_FS))+
geom_histogram(color = "black",
fill = "white", bins = sqrt(100))+
facet_wrap(~cluster_2,scales = 'free')+
theme_classic()# eine Gruppe ist schief!
leveneTest(facit_f_FS~cluster_2,data = df_corr_frontal)# nicht significant
# wilcox.test
wilcox.test(facit_f_FS~cluster_2, data = df_corr_frontal,
exact = FALSE,
correct = FALSE,
conf.int = FALSE)# 0.002
t.test(facit_f_FS~cluster_2, data = df_corr_frontal, alternative = "two.sided", paired = FALSE)# significant p = 0.001083
df_corr_frontal%>%
group_by(cluster_2)%>%
ggplot(aes(hads_d_total_score))+
geom_histogram(color = "black",
fill = "white", bins = sqrt(100))+
facet_wrap(~cluster_2,scales = 'free')+
theme_classic()# eine Gruppe ist schief
leveneTest(hads_d_total_score~cluster_2,data = df_corr_frontal)# not significant
t.test(hads_d_total_score~cluster_2, data = df_corr_frontal, alternative = "two.sided", paired = FALSE)# significant p = 0.001083
wilcox.test(hads_d_total_score~cluster_2, data = df_corr_frontal,
exact = FALSE,
correct = FALSE,
conf.int = FALSE)# without PCS hat eine schiefe Verteilung
df_corr_frontal%>%
group_by(cluster_2)%>%
ggplot(aes(tmt_a_time))+
geom_histogram(color = "black",
fill = "white", bins = sqrt(100))+
facet_wrap(~cluster_2,scales = 'free')+
theme_classic()# sieht okay aus
leveneTest(tmt_a_time~cluster_2,data = df_corr_frontal)# not significant
t.test(tmt_a_time~cluster_2, data = df_corr_frontal, alternative = "two.sided", paired = FALSE)# significant p = 0.001083
wilcox.test(tmt_a_time~cluster_2, data = df_corr_frontal,
exact = FALSE,
correct = FALSE,
conf.int = FALSE)
df_corr_frontal%>%
group_by(cluster_2)%>%
ggplot(aes(tmt_b_minus_a))+
geom_histogram(color = "black",
fill = "white", bins = sqrt(100))+
facet_wrap(~cluster_2,scales = 'free')+
theme_classic()# beide leicht schief (gleich)
leveneTest(tmt_b_minus_a~cluster_2,data = df_corr_frontal)# not significant
t.test(tmt_b_minus_a~cluster_2, data = df_corr_frontal, alternative = "two.sided", paired = FALSE)# significant p = 0.001083
wilcox.test(tmt_b_minus_a~cluster_2, data = df_corr_frontal,
exact = FALSE,
correct = FALSE,
conf.int = FALSE)# without PCS hat eine schiefe Verteilung
# number of epochs
df_corr_frontal%>%
group_by(cluster_2)%>%
summarise(mean_epoch = mean(number_epochs),
sd_age = sd(number_epochs))
t.test(number_epochs~cluster_2, data = df_corr_frontal, alternative = "two.sided", paired = FALSE)
df_corr_frontal%>%
ggplot(aes(x = number_epochs, y = mean_delta_power))+
geom_point()
cor.test(df_corr_frontal$mean_delta_power,df_corr_frontal$number_epochs)
df_corr_central%>%
ggplot(aes(x = number_epochs, y = mean_beta_power))+
geom_point()
cor.test(df_corr_central$mean_delta_power,df_corr_central$number_epochs)
# have a look at the whole data set (boxplots)
table_power_5%>%
group_by(cluster_2)%>%
ggplot(aes(x = cluster_2, y = rel_delta, color = cluster_2))+
geom_boxplot()
# per group
table_power_5%>%
group_by(cluster_2)%>%
ggplot(aes(x = cluster_2, y = rel_beta, color = cluster_2))+
geom_boxplot()
