cluster_2_data <- subset(clean_data, cluster == 2)
t_test_cluster_2 <- t.test(years_of_education ~ group, data = cluster_2_data)
t_test_cluster_2
# Cluster 3
cluster_3_data <- subset(clean_data, cluster == 3)
t_test_cluster_3 <- t.test(years_of_education ~ group, data = cluster_3_data)
t_test_cluster_3
# Cluster 4
cluster_4_data <- subset(clean_data, cluster == 4)
t_test_cluster_4 <- t.test(years_of_education ~ group, data = cluster_4_data)
t_test_cluster_4
# Vector of cognitive variables
variables <- c("pvt_reaction_time", "nback_miss_1", "nback_miss_2", "tmt_a_time", "tmt_b_time", "tmt_diff")
# Initialize an empty list to store the plots
plot_list <- list()
# Loop over each variable to create boxplots
for (variable in variables) {
plot <- ggplot(clean_data, aes(x = as.factor(cluster), y = !!sym(variable))) +
geom_boxplot(position = position_dodge(width = 0.75)) +
labs(x = "Cluster", y = variable, title = paste("Distribution of", variable, "by Cluster"))
plot_list[[variable]] <- plot
}
# Arrange plots in a grid
grid.arrange(grobs = plot_list, ncol = 2)
# Initialize lists to store results
anova_results <- list()
descriptive_stats_list <- list()
normality_results <- list()
homogeneity_results <- list()
effect_sizes <- list()
# Perform ANOVA, calculate descriptive statistics, check assumptions, and calculate effect sizes
for (variable in variables) {
# Perform ANOVA
anova_result <- aov(clean_data[[variable]] ~ as.factor(cluster), data = clean_data)
anova_results[[variable]] <- summary(anova_result)
# Calculate descriptive statistics
descriptive_stats <- clean_data %>%
group_by(cluster = as.factor(cluster)) %>%
summarise(
mean = round(mean(!!sym(variable), na.rm = TRUE), 2),
sd = round(sd(!!sym(variable), na.rm = TRUE), 2)
) %>%
mutate(
mean = format(mean, nsmall = 2),
sd = format(sd, nsmall = 2)
)
descriptive_stats_list[[variable]] <- descriptive_stats
# Check normality of residuals
# shapiro_test <- shapiro.test(residuals(lm(clean_data[[variable]] ~ as.factor(cluster), data = clean_data)))
#normality_results[[variable]] <- shapiro_test
# Check homogeneity of variances
#levene_test <- car::leveneTest(clean_data[[variable]] ~ as.factor(cluster), data = clean_data)
#homogeneity_results[[variable]] <- levene_test
# Calculate effect size (Eta Squared)
#eta_squared <- summary(anova_result)[[1]][["Sum Sq"]][1] / sum(summary(anova_result)[[1]][["Sum Sq"]])
#effect_sizes[[variable]] <- eta_squared
}
# Perform ANOVA, calculate descriptive statistics, check assumptions, and calculate effect sizes
for (variable in variables) {
# Perform ANOVA
anova_result <- aov(clean_data[[variable]] ~ as.factor(cluster), data = clean_data)
anova_results[[variable]] <- summary(anova_result)
# Calculate descriptive statistics
descriptive_stats <- clean_data %>%
group_by(cluster = as.factor(cluster)) %>%
summarise(
mean = round(mean(!!sym(variable), na.rm = TRUE), 2),
sd = round(sd(!!sym(variable), na.rm = TRUE), 2)
) %>%
mutate(
mean = format(mean, nsmall = 2),
sd = format(sd, nsmall = 2)
)
descriptive_stats_list[[variable]] <- descriptive_stats
# Check normality of residuals
shapiro_test <- shapiro.test(residuals(lm(clean_data[[variable]] ~ as.factor(cluster), data = clean_data)))
normality_results[[variable]] <- shapiro_test
# Check homogeneity of variances
levene_test <- car::leveneTest(clean_data[[variable]] ~ as.factor(cluster), data = clean_data)
homogeneity_results[[variable]] <- levene_test
# Calculate effect size (Eta Squared)
eta_squared <- summary(anova_result)[[1]][["Sum Sq"]][1] / sum(summary(anova_result)[[1]][["Sum Sq"]])
effect_sizes[[variable]] <- eta_squared
}
# Display results
anova_results
descriptive_stats_list
normality_results
homogeneity_results
effect_sizes
# Hierarchical clustering
library(tidyverse)
library(dplyr)
library(dendextend)
library(ggplot2)
library(gridExtra)
library(purrr)
library(vroom)
load("clean_data.RData")
# Extract relevant columns from clean_data
cog_df <- clean_data[, c("group","z_pvt_reaction_time_w","z_tmt_a_time_w","z_tmt_b_time_w")]
# Check structure and summarize contents of cog_df
str(cog_df)
summary(cog_df)
# Store group labels in a separate variable and exclude label (group column) from the dataset to do clustering
# Later true labels will be used to check how good clustering turned out
cog_label <- cog_df$group
cog_df$group <- NULL
str(cog_df)
# Build distance matrix
# Since all values are continuous numerical values I use euclidean distance method
dist_mat <- dist(cog_df, method = 'euclidean')
# Now decide which linkage method to use
# Try different kinds of linkage methods after decide which performed better
# Build dendrogram by plotting hierarchical cluster object with hclust
# Specify linkage method via 'method' argument
hclust_ward <- hclust(dist_mat, method = 'ward')
plot(hclust_ward)
# Create the desired number of clusters
# Since I want two groups 'withPCS' and 'withoutPCS' number of clusters = 2
cut_ward <- cutree(hclust_ward, k = 2)
# To visualize clusters on dendrogram use abline function to draw the cut line
plot(hclust_ward)
rect.hclust(hclust_ward, k = 2, border = 2:30)
abline(h = 28, col = 'red')
# Visualize tree with different colored branches
ward_dend_obj <- as.dendrogram(hclust_ward)
ward_col_dend <- color_branches(ward_dend_obj, h = 28)
plot(ward_col_dend)
# Append cluster results obtained back in the original dataframe
# Use mutate
# Count how many observations were assigned to each cluster with the count function
cog_df_cl <- mutate(cog_df, cluster = cut_ward)
count(cog_df_cl,cluster)
# Cross-checking clustering results using table function
table(cog_df_cl$cluster,cog_label)
# Add the cluster information from cog_df_cl to clean_data
clean_data$cluster <- cog_df_cl$cluster
# Check the updated structure of clean_data
str(clean_data)
# Checking sex in cluster
table(cog_df_cl$cluster,clean_data$sex)
cluster_info <- clean_data %>%
select(participant_id, cluster)
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/participants.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
merged_data <- merged_data %>%
rename(cluster_2 = cluster)
merged_data <- merged_data %>%
mutate(cluster_2 = recode(cluster_2, '1' = 'badperformer', '2' = 'goodperformer'))
# Check the structure of the merged data to ensure everything is correct
str(merged_data)
# Optionally, save the merged data to a new file
write_tsv(merged_data, "C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data.tsv")
View(merged_data)
load("clean_data.RData")
# Extract relevant columns from clean_data
cog_df <- clean_data[, c("group","z_pvt_reaction_time_w","z_tmt_a_time_w","z_tmt_b_time_w")]
# Check structure and summarize contents of cog_df
str(cog_df)
summary(cog_df)
# Store group labels in a separate variable and exclude label (group column) from the dataset to do clustering
# Later true labels will be used to check how good clustering turned out
cog_label <- cog_df$group
cog_df$group <- NULL
str(cog_df)
# Build distance matrix
# Since all values are continuous numerical values I use euclidean distance method
dist_mat <- dist(cog_df, method = 'euclidean')
# Now decide which linkage method to use
# Try different kinds of linkage methods after decide which performed better
# Build dendrogram by plotting hierarchical cluster object with hclust
# Specify linkage method via 'method' argument
hclust_ward <- hclust(dist_mat, method = 'ward')
plot(hclust_ward)
# Create the desired number of clusters
# Since I want two groups 'withPCS' and 'withoutPCS' number of clusters = 2
cut_ward <- cutree(hclust_ward, k = 4)
# To visualize clusters on dendrogram use abline function to draw the cut line
plot(hclust_ward)
rect.hclust(hclust_ward, k = 4, border = 2:30)
abline(h = 10, col = 'red')
# Visualize tree with different colored branches
ward_dend_obj <- as.dendrogram(hclust_ward)
ward_col_dend <- color_branches(ward_dend_obj, h = 10)
plot(ward_col_dend)
# Append cluster results obtained back in the original dataframe
# Use mutate
# Count how many observations were assigned to each cluster with the count function
cog_df_cl <- mutate(cog_df, cluster = cut_ward)
count(cog_df_cl,cluster)
# Cross-checking clustering results using table function
table(cog_df_cl$cluster,cog_label)
# Add the cluster information from cog_df_cl to clean_data
clean_data$cluster <- cog_df_cl$cluster
# Check the updated structure of clean_data
str(clean_data)
# Checking sex in cluster
table(cog_df_cl$cluster,clean_data$sex)
#--------
cluster_info <- clean_data %>%
select(participant_id, cluster)
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
merged_data <- merged_data %>%
rename(cluster_4 = cluster)
# Check the structure of the merged data to ensure everything is correct
str(merged_data)
# Optionally, save the merged data to a new file
write_tsv(merged_data, "C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data_all.tsv")
clear
clc
library(tidyverse)
library(dplyr)
# Import data
alldata <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/participants.tsv", na.strings = "n/a", header = TRUE)
# Subset of alldata that contains only the important variables
subset <- alldata %>%
select(participant_id,nr, age, sex, group, graduation, years_of_education, neurological_diseases_1, facit_f_FS, hads_a_total_score, hads_d_total_score, psqi_total_score, moca, pvt_reaction_time, nback_miss_1, nback_false_alarm_1 ,nback_miss_2 ,nback_false_alarm_2 ,tmt_a_time,tmt_b_time) %>%
mutate(tmt_diff = tmt_b_time - tmt_a_time)
# Convert 'moca' variable to a binary variable based on a cutoff score of 25
# 1 represents scores less than 26 (may) indicating cognitive impairment
# 0 represents scores more than 25 (may) indicating no cognitive impairment
subset <- subset %>%
mutate(moca = ifelse(moca > 25, 0, 1))
# Subset with all relevant cognitive data
cog_subset <- subset %>%
select(participant_id,nr, age, group, pvt_reaction_time, nback_miss_1, nback_false_alarm_1, nback_miss_2, nback_false_alarm_2, tmt_a_time, tmt_b_time)
# Summarize cog_subset
cog_subset %>%
summary()
# Summarize cog_subset
subset %>%
summary()
sex(F)
subset(sex)
sex
# Subset of alldata that contains only the important variables
subset <- alldata %>%
select(participant_id,nr, age, sex, group, graduation, years_of_education, neurological_diseases_1, facit_f_FS, hads_a_total_score, hads_d_total_score, psqi_total_score, moca, pvt_reaction_time, nback_miss_1, nback_false_alarm_1 ,nback_miss_2 ,nback_false_alarm_2 ,tmt_a_time,tmt_b_time) %>%
mutate(tmt_diff = tmt_b_time - tmt_a_time)
View(subset)
age
# Summarize cog_subset
cog_subset %>%
summary(sex=F)
summary(sex)
View(subset)
library(tidyverse)
library(dplyr)
# Import data
alldata <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/participants.tsv", na.strings = "n/a", header = TRUE)
# Subset of alldata that contains only the important variables
subset <- alldata %>%
select(participant_id,nr, age, sex, group, graduation, years_of_education, neurological_diseases_1, facit_f_FS, hads_a_total_score, hads_d_total_score, psqi_total_score, moca, pvt_reaction_time, nback_miss_1, nback_false_alarm_1 ,nback_miss_2 ,nback_false_alarm_2 ,tmt_a_time,tmt_b_time) %>%
mutate(tmt_diff = tmt_b_time - tmt_a_time)
# Convert 'moca' variable to a binary variable based on a cutoff score of 25
# 1 represents scores less than 26 (may) indicating cognitive impairment
# 0 represents scores more than 25 (may) indicating no cognitive impairment
subset <- subset %>%
mutate(moca = ifelse(moca > 25, 0, 1))
# Subset with all relevant cognitive data
cog_subset <- subset %>%
select(participant_id,nr, age, group, pvt_reaction_time, nback_miss_1, nback_false_alarm_1, nback_miss_2, nback_false_alarm_2, tmt_a_time, tmt_b_time)
# Summarize cog_subset
cog_subset %>%
summary()
# Summarize cog_subset
subset %>%
summary()
# Filter to get all "withPCS" rows
# Summary of "withPCS" group
cog_subset %>%
filter(group == "withPCS") %>%
summary(group == "withPCS")
# Filter to get all "withPCS" rows
# Summary of "withPCS" group
subset %>%
filter(group == "withPCS") %>%
summary(group == "withPCS")
# Filter to get all "withoutPCS" rows
# Summary of "withoutPCS" group
subset %>%
filter(group == "withoutPCS") %>%
summary(group == "withoutPCS")
View(subset)
# Checking for missing values
any(is.na(subset))
# Missing values = TRUE
# Removing rows with missing values
# cog_subset_clean <- cog_subset[complete.cases(cog_subset), ]
subset<- subset %>%
drop_na(pvt_reaction_time, nback_miss_1, nback_miss_2, tmt_a_time, tmt_b_time)
# Summarize cleaned dataframe
subset %>%
summary()
# Variables for which outliers are to be identified and winsorized
variables <- c("pvt_reaction_time","nback_miss_1","nback_miss_2","tmt_a_time","tmt_b_time")
# Initialize clean_data as a copy of subset
clean_data <- subset
# Function to winsorize a variable
winsorize_variable <- function(x) {
q1 <- quantile(x, 0.25)
q3 <- quantile(x, 0.75)
iqr <- IQR(x)
# Winsorization
x <- ifelse(x > q3 + 1.5 * iqr, q3 + 1.5 * iqr, x)
x <- ifelse(x < q1 - 1.5 * iqr, q1 - 1.5 * iqr, x)
x
}
# Loop over all variables
for (variable in variables) {
# Detect outliers
box_plot <- boxplot(subset[[variable]])$out
mtext(paste("Outliers for", variable, ":", paste(box_plot, collapse = ",")))
# Identify rows containing outliers
out_ind <- which(subset[[variable]] %in% c(box_plot))
cat("Indices of outliers for", variable, ":", out_ind, "\n")
cat("Rows with outliers for", variable, ":\n")
print(subset[out_ind,])
# Winsorize the variable and create a new column
winsorized_variable <- paste(variable, "w", sep = "_")
clean_data[[winsorized_variable]] <- winsorize_variable(subset[[variable]])
}
# Check the clean dataset
print(clean_data)
# Calculate tmt_diff_w
clean_data <- clean_data %>%
mutate(tmt_diff_w = tmt_b_time_w - tmt_a_time_w)
# Print clean_data to verify
print(clean_data)
# Test correlation
clean_data |>
select(pvt_reaction_time, nback_miss_1, nback_false_alarm_1, nback_miss_2, nback_false_alarm_2, tmt_a_time, tmt_b_time, tmt_diff) |>
cor(use = "pairwise.complete.obs") |>
round(2)
# Scatter plot of tmt_a_time vs. tmt_b_time faceted by age
ggplot(clean_data, aes(x = tmt_a_time, y = tmt_b_time, color = age)) +
geom_point()
# Scatter plot of nback_miss_1 vs. nback_miss_2 faceted by age
ggplot(clean_data, aes(x = nback_miss_1, y = nback_miss_2, color = age)) +
geom_point()
# Create age groups
clean_data <- clean_data %>%
mutate(
age_group = sapply(age, create_age_groups),
age_group_tmt_diff = sapply(age, create_age_groups_tmt_diff)
)
library(tidyverse)
library(dplyr)
# Import data
alldata <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/participants.tsv", na.strings = "n/a", header = TRUE)
# Subset of alldata that contains only the important variables
subset <- alldata %>%
select(participant_id,nr, age, sex, group, graduation, years_of_education, neurological_diseases_1, facit_f_FS, hads_a_total_score, hads_d_total_score, psqi_total_score, moca, pvt_reaction_time, nback_miss_1, nback_false_alarm_1 ,nback_miss_2 ,nback_false_alarm_2 ,tmt_a_time,tmt_b_time) %>%
mutate(tmt_diff = tmt_b_time - tmt_a_time)
# Convert 'moca' variable to a binary variable based on a cutoff score of 25
# 1 represents scores less than 26 (may) indicating cognitive impairment
# 0 represents scores more than 25 (may) indicating no cognitive impairment
subset <- subset %>%
mutate(moca = ifelse(moca > 25, 0, 1))
# Subset with all relevant cognitive data
cog_subset <- subset %>%
select(participant_id,nr, age, group, pvt_reaction_time, nback_miss_1, nback_false_alarm_1, nback_miss_2, nback_false_alarm_2, tmt_a_time, tmt_b_time)
# Summarize cog_subset
cog_subset %>%
summary()
# Filter to get all "withPCS" rows
# Summary of "withPCS" group
cog_subset %>%
filter(group == "withPCS") %>%
summary(group == "withPCS")
# Filter to get all "withoutPCS" rows
# Summary of "withoutPCS" group
cog_subset %>%
filter(group == "withoutPCS") %>%
summary(group == "withoutPCS")
# Checking for missing values
any(is.na(subset))
# Missing values = TRUE
# Removing rows with missing values
# cog_subset_clean <- cog_subset[complete.cases(cog_subset), ]
subset<- subset %>%
drop_na(pvt_reaction_time, nback_miss_1, nback_miss_2, tmt_a_time, tmt_b_time)
# Summarize cleaned dataframe
subset %>%
summary()
# Variables for which outliers are to be identified and winsorized
variables <- c("pvt_reaction_time","nback_miss_1","nback_miss_2","tmt_a_time","tmt_b_time")
# Initialize clean_data as a copy of subset
clean_data <- subset
# Function to winsorize a variable
winsorize_variable <- function(x) {
q1 <- quantile(x, 0.25)
q3 <- quantile(x, 0.75)
iqr <- IQR(x)
# Winsorization
x <- ifelse(x > q3 + 1.5 * iqr, q3 + 1.5 * iqr, x)
x <- ifelse(x < q1 - 1.5 * iqr, q1 - 1.5 * iqr, x)
x
}
# Loop over all variables
for (variable in variables) {
# Detect outliers
box_plot <- boxplot(subset[[variable]])$out
mtext(paste("Outliers for", variable, ":", paste(box_plot, collapse = ",")))
# Identify rows containing outliers
out_ind <- which(subset[[variable]] %in% c(box_plot))
cat("Indices of outliers for", variable, ":", out_ind, "\n")
cat("Rows with outliers for", variable, ":\n")
print(subset[out_ind,])
# Winsorize the variable and create a new column
winsorized_variable <- paste(variable, "w", sep = "_")
clean_data[[winsorized_variable]] <- winsorize_variable(subset[[variable]])
}
# Check the clean dataset
print(clean_data)
# Calculate tmt_diff_w
clean_data <- clean_data %>%
mutate(tmt_diff_w = tmt_b_time_w - tmt_a_time_w)
# Print clean_data to verify
print(clean_data)
# Test correlation
clean_data |>
select(pvt_reaction_time, nback_miss_1, nback_false_alarm_1, nback_miss_2, nback_false_alarm_2, tmt_a_time, tmt_b_time, tmt_diff) |>
cor(use = "pairwise.complete.obs") |>
round(2)
# Scatter plot of tmt_a_time vs. tmt_b_time faceted by age
ggplot(clean_data, aes(x = tmt_a_time, y = tmt_b_time, color = age)) +
geom_point()
# Scatter plot of nback_miss_1 vs. nback_miss_2 faceted by age
ggplot(clean_data, aes(x = nback_miss_1, y = nback_miss_2, color = age)) +
geom_point()
# Function to create age groups
create_age_groups <- function(age) {
if (age >= 18 & age <= 34) {
return("18-34 Years")
} else if (age >= 35 & age <= 49) {
return("35-49 Years")
} else if (age >= 50 & age <= 64) {
return("50-64 Years")
} else if (age >= 65 & age <= 80) {
return("65-80 Years")
}
}
# Function to create age groups for tmt_diff
create_age_groups_tmt_diff <- function(age) {
if (age >= 18 & age <= 24) {
return("18-24 Years")
} else if (age >= 25 & age <= 54) {
return("25-54 Years")
} else if (age >= 55 & age <= 64) {
return("55-64 Years")
} else if (age >= 65 & age <= 80) {
return("65-80 Years")
}
}
# Create age groups
clean_data <- clean_data %>%
mutate(
age_group = sapply(age, create_age_groups),
age_group_tmt_diff = sapply(age, create_age_groups_tmt_diff)
)
# Calculate mean and standard deviation for each age group and variable
age_group_summary <- clean_data %>%
group_by(age_group) %>%
summarize(across(c("pvt_reaction_time_w", "tmt_a_time_w", "tmt_b_time_w"),
list(mean = mean, sd = sd)))
age_group_summary_tmt_diff <- clean_data %>%
group_by(age_group_tmt_diff) %>%
summarize(across(c("tmt_diff_w"),
list(mean = mean, sd = sd)))
View(age_group_summary)
View(clean_data)
# Function to calculate z-scores
calculate_z_scores <- function(x, mean, sd) {
(x - mean) / sd
}
# Function to calculate z-scores for each individual based on age for all variables
calculate_z_scores_individual <- function(x, age, age_group_summary, age_group_summary_tmt_diff) {
# Find the corresponding age group for each individual
age_group <- sapply(age, create_age_groups)
age_group_tmt_diff <- sapply(age, create_age_groups_tmt_diff)
# Join the age group summary data to the individual data based on age group
individual_data <- data.frame(x, age, age_group) %>%
left_join(age_group_summary, by = "age_group") %>%
left_join(age_group_summary_tmt_diff, by = "age_group_tmt_diff")
# Calculate z-scores for all variables
z_scores <- individual_data %>%
mutate(
z_pvt_reaction_time_w = calculate_z_scores(pvt_reaction_time_w, pvt_reaction_time_w_mean, pvt_reaction_time_w_sd),
z_tmt_a_time_w = calculate_z_scores(tmt_a_time_w, tmt_a_time_w_mean, tmt_a_time_w_sd),
z_tmt_b_time_w = calculate_z_scores(tmt_b_time_w, tmt_b_time_w_mean, tmt_b_time_w_sd),
z_tmt_diff_w = calculate_z_scores(tmt_diff_w, tmt_diff_w_mean, tmt_diff_w_sd)
) %>%
select(starts_with("z_"))
# Combine the Z-scores with the original data
x <- cbind(x, z_scores)
return(x)
}
# Calculate z-scores for each individual based on age for all variables
clean_data <- calculate_z_scores_individual(clean_data, clean_data$age, age_group_summary, age_group_summary_tmt_diff)
