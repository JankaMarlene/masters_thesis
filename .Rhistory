summary(cog_df)
# Store group labels in a separate variable and exclude label (group column) from the dataset to do clustering
# Later true labels will be used to check how good clustering turned out
cog_label <- cog_df$group
cog_df$group <- NULL
str(cog_df)
# Build distance matrix
# Since all values are continuous numerical values I use euclidean distance method
dist_mat <- dist(cog_df, method = 'euclidean')
# Now decide which linkage method to use
# Try different kinds of linkage methods after decide which performed better
# Build dendrogram by plotting hierarchical cluster object with hclust
# Specify linkage method via 'method' argument
hclust_ward <- hclust(dist_mat, method = 'ward')
plot(hclust_ward)
# Create the desired number of clusters
# Since I want two groups 'withPCS' and 'withoutPCS' number of clusters = 2
cut_ward <- cutree(hclust_ward, k = 2)
# To visualize clusters on dendrogram use abline function to draw the cut line
plot(hclust_ward)
rect.hclust(hclust_ward, k = 2, border = 2:30)
abline(h = 28, col = 'red')
# Visualize tree with different colored branches
ward_dend_obj <- as.dendrogram(hclust_ward)
ward_col_dend <- color_branches(ward_dend_obj, h = 28)
plot(ward_col_dend)
# Visualize the clusters see YT Video Hierarchical Clustering in R Spencer Pao
# install.packages("factoextra")
# library(factoextra)
# cluster_obj <- list(data = cog_df_sc, cluster = cut_avg)
# fviz_cluster(cluster_obj)
# rownames(cog_df_sc) <- paste(cog_label, 1:dim(cog_df) [1], sep = "_")
# fviz_cluster(list(data=cog_df_sc))
# Append cluster results obtained back in the original dataframe
# Use mutate
# Count how many observations were assigned to each cluster with the count function
cog_df_cl <- mutate(cog_df, cluster = cut_ward)
count(cog_df_cl,cluster)
# Cross-checking clustering results using table function
table(cog_df_cl$cluster,cog_label)
# Add the cluster information from cog_df_cl to clean_data
clean_data$cluster <- cog_df_cl$cluster
# Check the updated structure of clean_data
str(clean_data)
# Checking sex in cluster
table(cog_df_cl$cluster,clean_data$sex)
cluster_info <- clean_data %>%
select(participant_id, cluster)
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/participants.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
rename(cluster_2 = cluster)
library(tidyverse)
library(dplyr)
# Import data
alldata <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/participants.tsv", na.strings = "n/a", header = TRUE)
# Subset of alldata that contains only the important variables
subset <- alldata %>%
select(participant_id,nr, age, sex, group, graduation, years_of_education, neurological_diseases_1, facit_f_FS, hads_a_total_score, hads_d_total_score, psqi_total_score, moca, pvt_reaction_time, nback_miss_1, nback_false_alarm_1 ,nback_miss_2 ,nback_false_alarm_2 ,tmt_a_time,tmt_b_time) %>%
mutate(tmt_diff = tmt_b_time - tmt_a_time)
# Adding the TMT difference
# Convert 'moca' variable to a binary variable based on a cutoff score of 25
# 1 represents scores less than 26 (may) indicating cognitive impairment
# 0 represents scores more than 25 (may) indicating no cognitive impairment
subset <- subset %>%
mutate(moca = ifelse(moca > 25, 0, 1))
# Subset with all relevant cognitive data
cog_subset <- subset %>%
select(participant_id,nr, age, group, pvt_reaction_time, nback_miss_1, nback_false_alarm_1, nback_miss_2, nback_false_alarm_2, tmt_a_time, tmt_b_time)
# Summarize cog_subset
cog_subset %>%
summary()
# Filter to get all "withPCS" rows
# Summary of "withPCS" group
cog_subset %>%
filter(group == "withPCS") %>%
summary(group == "withPCS")
# Filter to get all "withoutPCS" rows
# Summary of "withoutPCS" group
cog_subset %>%
filter(group == "withoutPCS") %>%
summary(group == "withoutPCS")
# Checking for missing values
any(is.na(subset))
# Missing values = TRUE
# Removing rows with missing values
# cog_subset_clean <- cog_subset[complete.cases(cog_subset), ]
subset<- subset %>%
drop_na(pvt_reaction_time, nback_miss_1, nback_miss_2, tmt_a_time, tmt_b_time)
# Summarize cleaned dataframe
subset %>%
summary()
# Variables for which outliers are to be identified and winsorized
variables <- c("pvt_reaction_time","nback_miss_1","nback_miss_2","tmt_a_time","tmt_b_time")
# Initialize clean_data as a copy of subset
clean_data <- subset
# Function to winsorize a variable
winsorize_variable <- function(x) {
q1 <- quantile(x, 0.25)
q3 <- quantile(x, 0.75)
iqr <- IQR(x)
# Winsorization
x <- ifelse(x > q3 + 1.5 * iqr, q3 + 1.5 * iqr, x)
x <- ifelse(x < q1 - 1.5 * iqr, q1 - 1.5 * iqr, x)
x
}
# Loop over all variables
for (variable in variables) {
# Detect outliers
box_plot <- boxplot(subset[[variable]])$out
mtext(paste("Outliers for", variable, ":", paste(box_plot, collapse = ",")))
# Identify rows containing outliers
out_ind <- which(subset[[variable]] %in% c(box_plot))
cat("Indices of outliers for", variable, ":", out_ind, "\n")
cat("Rows with outliers for", variable, ":\n")
print(subset[out_ind,])
# Winsorize the variable and create a new column
winsorized_variable <- paste(variable, "w", sep = "_")
clean_data[[winsorized_variable]] <- winsorize_variable(subset[[variable]])
}
# Check the clean dataset
print(clean_data)
# Calculate tmt_diff_w
clean_data <- clean_data %>%
mutate(tmt_diff_w = tmt_b_time_w - tmt_a_time_w)
# Print clean_data to verify
print(clean_data)
# Loop over all variables
#for (variable in variables) {
# Detect outliers
#box_plot <- boxplot(subset[[variable]])$out
#mtext(paste("Outliers for", variable, ":", paste(box_plot,
#collapse = ",")))
# Identify rows containing outliers
#out_ind <- which(subset[[variable]] %in% c(box_plot))
#cat("Indices of outliers for", variable, ":", out_ind, "\n")
#cat("Rows with outliers for", variable, ":\n")
#print(subset[out_ind,])
# Remove outliers
#clean_data <- subset[!subset[[variable]] %in% box_plot, ]
#}
# Test correlation
clean_data |>
select(pvt_reaction_time, nback_miss_1, nback_false_alarm_1, nback_miss_2, nback_false_alarm_2, tmt_a_time, tmt_b_time, tmt_diff) |>
cor(use = "pairwise.complete.obs") |>
round(2)
# Scatter plot of tmt_a_time vs. tmt_b_time faceted by age
ggplot(clean_data, aes(x = tmt_a_time, y = tmt_b_time, color = age)) +
geom_point()
# Scatter plot of nback_miss_1 vs. nback_miss_2 faceted by age
ggplot(clean_data, aes(x = nback_miss_1, y = nback_miss_2, color = age)) +
geom_point()
# Function to create age groups
create_age_groups <- function(age) {
if (age >= 18 & age <= 34) {
return("18-34 Years")
} else if (age >= 35 & age <= 49) {
return("35-49 Years")
} else if (age >= 50 & age <= 64) {
return("50-64 Years")
} else if (age >= 65 & age <= 80) {
return("65-80 Years")
}
}
# Function to create age groups for tmt_diff
create_age_groups_tmt_diff <- function(age) {
if (age >= 18 & age <= 24) {
return("18-24 Years")
} else if (age >= 25 & age <= 54) {
return("25-54 Years")
} else if (age >= 55 & age <= 64) {
return("55-64 Years")
} else if (age >= 65 & age <= 80) {
return("65-80 Years")
}
}
# Create age groups
clean_data <- clean_data %>%
mutate(
age_group = sapply(age, create_age_groups),
age_group_tmt_diff = sapply(age, create_age_groups_tmt_diff)
)
# Calculate mean and standard deviation for each age group and variable
age_group_summary <- clean_data %>%
group_by(age_group) %>%
summarize(across(c("pvt_reaction_time_w", "tmt_a_time_w", "tmt_b_time_w"),
list(mean = mean, sd = sd)))
age_group_summary_tmt_diff <- clean_data %>%
group_by(age_group_tmt_diff) %>%
summarize(across(c("tmt_diff_w"),
list(mean = mean, sd = sd)))
# Function to calculate z-scores
calculate_z_scores <- function(x, mean, sd) {
(x - mean) / sd
}
# Function to calculate z-scores for each individual based on age for all variables
calculate_z_scores_individual <- function(x, age, age_group_summary, age_group_summary_tmt_diff) {
# Find the corresponding age group for each individual
age_group <- sapply(age, create_age_groups)
age_group_tmt_diff <- sapply(age, create_age_groups_tmt_diff)
# Join the age group summary data to the individual data based on age group
individual_data <- data.frame(x, age, age_group) %>%
left_join(age_group_summary, by = "age_group") %>%
left_join(age_group_summary_tmt_diff, by = "age_group_tmt_diff")
# Calculate z-scores for all variables
z_scores <- individual_data %>%
mutate(
z_pvt_reaction_time_w = calculate_z_scores(pvt_reaction_time_w, pvt_reaction_time_w_mean, pvt_reaction_time_w_sd),
z_tmt_a_time_w = calculate_z_scores(tmt_a_time_w, tmt_a_time_w_mean, tmt_a_time_w_sd),
z_tmt_b_time_w = calculate_z_scores(tmt_b_time_w, tmt_b_time_w_mean, tmt_b_time_w_sd),
z_tmt_diff_w = calculate_z_scores(tmt_diff_w, tmt_diff_w_mean, tmt_diff_w_sd)
) %>%
select(starts_with("z_"))
# Combine the Z-scores with the original data
x <- cbind(x, z_scores)
return(x)
}
# Calculate z-scores for each individual based on age for all variables
clean_data <- calculate_z_scores_individual(clean_data, clean_data$age, age_group_summary, age_group_summary_tmt_diff)
# Scale the variables nback_miss_1 and nback_miss_2 and add them as new columns
clean_data <- clean_data %>%
mutate(
s_nback_miss_1_w = as.vector(scale(nback_miss_1_w)),
s_nback_miss_2_w = as.vector(scale(nback_miss_2_w))
)
# clean_data[, c("nback_miss_1","nback_miss_2")] = scale(clean_data[, c("nback_miss_1","nback_miss_2")])
save(clean_data, file = "clean_data.RData")
# Hierarchical clustering
library(tidyverse)
library(dplyr)
library(dendextend)
library(ggplot2)
library(gridExtra)
library(purrr)
library(vroom)
load("clean_data.RData")
# Extract relevant columns from clean_data
cog_df <- clean_data[, c("group","z_pvt_reaction_time_w","z_tmt_a_time_w","z_tmt_b_time_w")]
# Check structure and summarize contents of cog_df
str(cog_df)
summary(cog_df)
# Store group labels in a separate variable and exclude label (group column) from the dataset to do clustering
# Later true labels will be used to check how good clustering turned out
cog_label <- cog_df$group
cog_df$group <- NULL
str(cog_df)
# Build distance matrix
# Since all values are continuous numerical values I use euclidean distance method
dist_mat <- dist(cog_df, method = 'euclidean')
# Now decide which linkage method to use
# Try different kinds of linkage methods after decide which performed better
# Build dendrogram by plotting hierarchical cluster object with hclust
# Specify linkage method via 'method' argument
hclust_ward <- hclust(dist_mat, method = 'ward')
plot(hclust_ward)
# Create the desired number of clusters
# Since I want two groups 'withPCS' and 'withoutPCS' number of clusters = 2
cut_ward <- cutree(hclust_ward, k = 2)
# To visualize clusters on dendrogram use abline function to draw the cut line
plot(hclust_ward)
rect.hclust(hclust_ward, k = 2, border = 2:30)
abline(h = 28, col = 'red')
# Visualize tree with different colored branches
ward_dend_obj <- as.dendrogram(hclust_ward)
ward_col_dend <- color_branches(ward_dend_obj, h = 28)
plot(ward_col_dend)
# Visualize the clusters see YT Video Hierarchical Clustering in R Spencer Pao
# install.packages("factoextra")
# library(factoextra)
# cluster_obj <- list(data = cog_df_sc, cluster = cut_avg)
# fviz_cluster(cluster_obj)
# rownames(cog_df_sc) <- paste(cog_label, 1:dim(cog_df) [1], sep = "_")
# fviz_cluster(list(data=cog_df_sc))
# Append cluster results obtained back in the original dataframe
# Use mutate
# Count how many observations were assigned to each cluster with the count function
cog_df_cl <- mutate(cog_df, cluster = cut_ward)
count(cog_df_cl,cluster)
# Cross-checking clustering results using table function
table(cog_df_cl$cluster,cog_label)
# Add the cluster information from cog_df_cl to clean_data
clean_data$cluster <- cog_df_cl$cluster
# Check the updated structure of clean_data
str(clean_data)
# Checking sex in cluster
table(cog_df_cl$cluster,clean_data$sex)
#--------
cluster_info <- clean_data %>%
select(participant_id, cluster)
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/participants.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
rename(cluster_2 = cluster)
View(cluster_info)
cluster_info <- clean_data %>%
select(participant_id, cluster)
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/participants.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
merged_data <- merged_data %>%
rename(cluster_2 = cluster)
# Check the structure of the merged data to ensure everything is correct
str(merged_data)
# Optionally, save the merged data to a new file
write_tsv(merged_data, "C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data.tsv")
View(cluster_info)
# Hierarchical clustering
library(tidyverse)
library(dplyr)
library(dendextend)
library(ggplot2)
library(gridExtra)
library(purrr)
library(car)
load("clean_data.RData")
# Extract relevant columns from clean_data
cog_df <- clean_data[, c("group","z_pvt_reaction_time_w","z_tmt_a_time_w","z_tmt_b_time_w")]
# Check structure and summarize contents of cog_df
str(cog_df)
summary(cog_df)
# Store group labels in a separate variable and exclude label (group column) from the dataset to do clustering
# Later true labels will be used to check how good clustering turned out
cog_label <- cog_df$group
cog_df$group <- NULL
str(cog_df)
# Build distance matrix
# Since all values are continuous numerical values I use euclidean distance method
dist_mat <- dist(cog_df, method = 'euclidean')
# Now decide which linkage method to use
# Try different kinds of linkage methods after decide which performed better
# Build dendrogram by plotting hierarchical cluster object with hclust
# Specify linkage method via 'method' argument
hclust_ward <- hclust(dist_mat, method = 'ward')
plot(hclust_ward)
# Create the desired number of clusters
# Since I want two groups 'withPCS' and 'withoutPCS' number of clusters = 2
cut_ward <- cutree(hclust_ward, k = 4)
# To visualize clusters on dendrogram use abline function to draw the cut line
plot(hclust_ward)
rect.hclust(hclust_ward, k = 4, border = 2:30)
abline(h = 10, col = 'red')
# Visualize tree with different colored branches
ward_dend_obj <- as.dendrogram(hclust_ward)
ward_col_dend <- color_branches(ward_dend_obj, h = 10)
plot(ward_col_dend)
# Append cluster results obtained back in the original dataframe
# Use mutate
# Count how many observations were assigned to each cluster with the count function
cog_df_cl <- mutate(cog_df, cluster = cut_ward)
count(cog_df_cl,cluster)
# Cross-checking clustering results using table function
table(cog_df_cl$cluster,cog_label)
# Add the cluster information from cog_df_cl to clean_data
clean_data$cluster <- cog_df_cl$cluster
# Check the updated structure of clean_data
str(clean_data)
# Checking sex in cluster
table(cog_df_cl$cluster,clean_data$sex)
#--------
cluster_info <- clean_data %>%
select(participant_id, cluster)
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
merged_data <- merged_data %>%
rename(cluster_4 = cluster)
# Check the structure of the merged data to ensure everything is correct
str(merged_data)
# Optionally, save the merged data to a new file
write_tsv(merged_data, "C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data_all.tsv")
test_table%>%
group_by(cluster_2)%>%
summarise(mean_age = mean(age),
sd_age = sd(age),
mean_epochs = mean(number_epochs))# sind immerhin fast identisch vom Alter her
#-------3. summarise mean -----------------
df_corr_frontal <- table_power_frontal%>%
group_by(participant_id,group,tmt_a_time,facit_f_FS, tmt_b_minus_a,age,years_of_education,sex,hads_d_total_score, number_epochs, cluster_2)%>%
summarise(mean_delta_power = mean(rel_delta),
mean_beta_power = mean(rel_beta),
mean_aperiodic_exponent = mean(aperiodic_exponent))
df_corr_central <- table_power_central%>%
group_by(participant_id,group,tmt_a_time,facit_f_FS, tmt_b_minus_a,hads_d_total_score, number_epochs, cluster_2)%>%
summarise(mean_delta_power = mean(rel_delta),
mean_beta_power = mean(rel_beta),
mean_aperiodic_exponent = mean(aperiodic_exponent))
library(tidyverse)
library(carData)
library(car)
library(readr)
library(ggdist)
library(ggExtra)# displaying distributions next to plots
library(ggsignif)# displaying stats in plots
library(ggplot2)
library(ggpubr)
table_power_5 <- read_csv("data/analysis_power/table_power_final_5.csv")
number_of_epochs_5 <- read_csv("data/analysis_power/number_of_epochs_5.csv")
setwd("C:/Users/jankj/OneDrive/Desktop/masters_thesis")
table_power_5 <- read_csv("data/analysis_power/table_power_final_5.csv")
number_of_epochs_5 <- read_csv("data/analysis_power/number_of_epochs_5.csv")
View(number_of_epochs_5)
View(table_power_5)
table_power_5 <- read_csv("data/analysis_power/table_power_final_5.csv")
number_of_epochs_5 <- read_csv("data/analysis_power/number_of_epochs_5.csv")
# this would be the amount of people where more than half of the data is good
number_of_epochs%>%
filter(number_epochs > 37.5)# here 70
number_of_epochs%>%
filter(number_epochs < 51)# these subjects have to be excluded
number_of_epochs_5%>%
filter(number_epochs > 37.5)# here 62
number_of_epochs_5%>%
filter(number_epochs < 37.5)# these subjects have to be excluded
# modify table (f.ex. add tmt b-a)
table_power_5 <- table_power_5%>%
mutate(facit_f_FS = as.numeric(facit_f_FS),
tmt_b_minus_a = tmt_b_time-tmt_a_time)
table_power_5 <- merge(table_power_5, number_of_epochs_5)
test_table <- table_power_5%>%
group_by(participant_id,group,tmt_a_time,facit_f_FS, tmt_b_minus_a,age,years_of_education,number_epochs,cluster_2)%>%
summarise(mean_delta_power = mean(rel_delta),
mean_beta_power = mean(rel_beta),
mean_aperiodic_exponent = mean(aperiodic_exponent))
View(test_table)
clear
clc
library(tidyverse)
library(carData)
library(car)
library(readr)
library(ggdist)
library(ggExtra)# displaying distributions next to plots
library(ggsignif)# displaying stats in plots
library(ggplot2)
library(ggpubr)
table_power_5 <- read_csv("data/analysis_power/table_power_final_5.csv")
number_of_epochs_5 <- read_csv("data/analysis_power/number_of_epochs_5.csv")
# this would be the amount of people where more than half of the data is good
number_of_epochs%>%
filter(number_epochs > 37.5)# here 70
number_of_epochs%>%
filter(number_epochs > 50)# here 61
number_of_epochs%>%
filter(number_epochs < 51)# these subjects have to be excluded
number_of_epochs_5%>%
filter(number_epochs > 37.5)# here 62
number_of_epochs_5%>%
filter(number_epochs < 37.5)# these subjects have to be excluded
# modify table (f.ex. add tmt b-a)
table_power_5 <- table_power_5%>%
mutate(facit_f_FS = as.numeric(facit_f_FS),
tmt_b_minus_a = tmt_b_time-tmt_a_time)
table_power_5 <- merge(table_power_5, number_of_epochs_5)
test_table <- table_power_5%>%
group_by(participant_id,group,tmt_a_time,facit_f_FS, tmt_b_minus_a,age,years_of_education,number_epochs,cluster_2)%>%
summarise(mean_delta_power = mean(rel_delta),
mean_beta_power = mean(rel_beta),
mean_aperiodic_exponent = mean(aperiodic_exponent))
View(test_table)
write_csv(test_table, "C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/test_table.csv")
install.packages("flextable")
install.packages("officer")
library(flextable)
# Create the data frame
data <- data.frame(
Group = c("Total (n = 79)", "With PCS (n = 49)", "Without PCS (n = 30)"),
Mean_Age = c(48.52, 50.29, 45.63),
Age_Range = c("22-78", "22-78", "22-77"),
Female_n = c(48, 32, 16),
Male_n = c(31, 17, 14),
Mean_Education = c(15.27, 15.04, 15.63),
Education_Range = c("9-24", "9-23", "10-24"),
Diverse_n = c(0, 0, 0)
)
# Create and format the flextable
ft <- flextable(data) %>%
set_header_labels(
Group = "Group",
Mean_Age = "Mean Age (years)",
Age_Range = "Age Range",
Female_n = "Female (n)",
Male_n = "Male (n)",
Mean_Education = "Mean Education (years)",
Education_Range = "Education Range",
Diverse_n = "Diverse (n)"
) %>%
theme_vanilla() %>%
autofit()
# Create a Word document and add the table
doc <- read_docx() %>%
body_add_flextable(value = ft)
library(flextable)
# Install and load the necessary packages
install.packages("flextable")
install.packages("officer")
library(flextable)
