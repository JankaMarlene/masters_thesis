clean_data <- subset
# Function to winsorize a variable
winsorize_variable <- function(x) {
q1 <- quantile(x, 0.25)
q3 <- quantile(x, 0.75)
iqr <- IQR(x)
# Winsorization
x <- ifelse(x > q3 + 1.5 * iqr, q3 + 1.5 * iqr, x)
x <- ifelse(x < q1 - 1.5 * iqr, q1 - 1.5 * iqr, x)
x
}
# Loop over all variables
for (variable in variables) {
# Detect outliers
box_plot <- boxplot(subset[[variable]])$out
mtext(paste("Outliers for", variable, ":", paste(box_plot, collapse = ",")))
# Identify rows containing outliers
out_ind <- which(subset[[variable]] %in% c(box_plot))
cat("Indices of outliers for", variable, ":", out_ind, "\n")
cat("Rows with outliers for", variable, ":\n")
print(subset[out_ind,])
# Winsorize the variable and create a new column
winsorized_variable <- paste(variable, "w", sep = "_")
clean_data[[winsorized_variable]] <- winsorize_variable(subset[[variable]])
}
# Check the clean dataset
print(clean_data)
# Calculate tmt_diff_w
clean_data <- clean_data %>%
mutate(tmt_diff_w = tmt_b_time_w - tmt_a_time_w)
# Print clean_data to verify
print(clean_data)
# Loop over all variables
#for (variable in variables) {
# Detect outliers
#box_plot <- boxplot(subset[[variable]])$out
#mtext(paste("Outliers for", variable, ":", paste(box_plot,
#collapse = ",")))
# Identify rows containing outliers
#out_ind <- which(subset[[variable]] %in% c(box_plot))
#cat("Indices of outliers for", variable, ":", out_ind, "\n")
#cat("Rows with outliers for", variable, ":\n")
#print(subset[out_ind,])
# Remove outliers
#clean_data <- subset[!subset[[variable]] %in% box_plot, ]
#}
# Test correlation
clean_data |>
select(pvt_reaction_time, nback_miss_1, nback_false_alarm_1, nback_miss_2, nback_false_alarm_2, tmt_a_time, tmt_b_time, tmt_diff) |>
cor(use = "pairwise.complete.obs") |>
round(2)
# Scatter plot of tmt_a_time vs. tmt_b_time faceted by age
ggplot(clean_data, aes(x = tmt_a_time, y = tmt_b_time, color = age)) +
geom_point()
# Scatter plot of nback_miss_1 vs. nback_miss_2 faceted by age
ggplot(clean_data, aes(x = nback_miss_1, y = nback_miss_2, color = age)) +
geom_point()
# Function to create age groups
create_age_groups <- function(age) {
if (age >= 18 & age <= 34) {
return("18-34 Years")
} else if (age >= 35 & age <= 49) {
return("35-49 Years")
} else if (age >= 50 & age <= 64) {
return("50-64 Years")
} else if (age >= 65 & age <= 80) {
return("65-80 Years")
}
}
# Function to create age groups for tmt_diff
create_age_groups_tmt_diff <- function(age) {
if (age >= 18 & age <= 24) {
return("18-24 Years")
} else if (age >= 25 & age <= 54) {
return("25-54 Years")
} else if (age >= 55 & age <= 64) {
return("55-64 Years")
} else if (age >= 65 & age <= 80) {
return("65-80 Years")
}
}
# Create age groups
clean_data <- clean_data %>%
mutate(
age_group = sapply(age, create_age_groups),
age_group_tmt_diff = sapply(age, create_age_groups_tmt_diff)
)
# Calculate mean and standard deviation for each age group and variable
age_group_summary <- clean_data %>%
group_by(age_group) %>%
summarize(across(c("pvt_reaction_time_w", "tmt_a_time_w", "tmt_b_time_w"),
list(mean = mean, sd = sd)))
age_group_summary_tmt_diff <- clean_data %>%
group_by(age_group_tmt_diff) %>%
summarize(across(c("tmt_diff_w"),
list(mean = mean, sd = sd)))
# Function to calculate z-scores
calculate_z_scores <- function(x, mean, sd) {
(x - mean) / sd
}
# Function to calculate z-scores for each individual based on age for all variables
calculate_z_scores_individual <- function(x, age, age_group_summary, age_group_summary_tmt_diff) {
# Find the corresponding age group for each individual
age_group <- sapply(age, create_age_groups)
age_group_tmt_diff <- sapply(age, create_age_groups_tmt_diff)
# Join the age group summary data to the individual data based on age group
individual_data <- data.frame(x, age, age_group) %>%
left_join(age_group_summary, by = "age_group") %>%
left_join(age_group_summary_tmt_diff, by = "age_group_tmt_diff")
# Calculate z-scores for all variables
z_scores <- individual_data %>%
mutate(
z_pvt_reaction_time_w = calculate_z_scores(pvt_reaction_time_w, pvt_reaction_time_w_mean, pvt_reaction_time_w_sd),
z_tmt_a_time_w = calculate_z_scores(tmt_a_time_w, tmt_a_time_w_mean, tmt_a_time_w_sd),
z_tmt_b_time_w = calculate_z_scores(tmt_b_time_w, tmt_b_time_w_mean, tmt_b_time_w_sd),
z_tmt_diff_w = calculate_z_scores(tmt_diff_w, tmt_diff_w_mean, tmt_diff_w_sd)
) %>%
select(starts_with("z_"))
# Combine the Z-scores with the original data
x <- cbind(x, z_scores)
return(x)
}
# Calculate z-scores for each individual based on age for all variables
clean_data <- calculate_z_scores_individual(clean_data, clean_data$age, age_group_summary, age_group_summary_tmt_diff)
# Scale the variables nback_miss_1 and nback_miss_2 and add them as new columns
clean_data <- clean_data %>%
mutate(
s_nback_miss_1_w = as.vector(scale(nback_miss_1_w)),
s_nback_miss_2_w = as.vector(scale(nback_miss_2_w))
)
# clean_data[, c("nback_miss_1","nback_miss_2")] = scale(clean_data[, c("nback_miss_1","nback_miss_2")])
save(clean_data, file = "clean_data.RData")
# Hierarchical clustering
library(tidyverse)
library(dplyr)
library(dendextend)
library(ggplot2)
library(gridExtra)
library(purrr)
library(vroom)
load("clean_data.RData")
# Extract relevant columns from clean_data
cog_df <- clean_data[, c("group","z_pvt_reaction_time_w","z_tmt_a_time_w","z_tmt_b_time_w")]
# Check structure and summarize contents of cog_df
str(cog_df)
summary(cog_df)
# Store group labels in a separate variable and exclude label (group column) from the dataset to do clustering
# Later true labels will be used to check how good clustering turned out
cog_label <- cog_df$group
cog_df$group <- NULL
str(cog_df)
# Build distance matrix
# Since all values are continuous numerical values I use euclidean distance method
dist_mat <- dist(cog_df, method = 'euclidean')
# Now decide which linkage method to use
# Try different kinds of linkage methods after decide which performed better
# Build dendrogram by plotting hierarchical cluster object with hclust
# Specify linkage method via 'method' argument
hclust_ward <- hclust(dist_mat, method = 'ward')
plot(hclust_ward)
# Create the desired number of clusters
# Since I want two groups 'withPCS' and 'withoutPCS' number of clusters = 2
cut_ward <- cutree(hclust_ward, k = 2)
# To visualize clusters on dendrogram use abline function to draw the cut line
plot(hclust_ward)
rect.hclust(hclust_ward, k = 2, border = 2:30)
abline(h = 28, col = 'red')
# Visualize tree with different colored branches
ward_dend_obj <- as.dendrogram(hclust_ward)
ward_col_dend <- color_branches(ward_dend_obj, h = 28)
plot(ward_col_dend)
# Visualize the clusters see YT Video Hierarchical Clustering in R Spencer Pao
# install.packages("factoextra")
# library(factoextra)
# cluster_obj <- list(data = cog_df_sc, cluster = cut_avg)
# fviz_cluster(cluster_obj)
# rownames(cog_df_sc) <- paste(cog_label, 1:dim(cog_df) [1], sep = "_")
# fviz_cluster(list(data=cog_df_sc))
# Append cluster results obtained back in the original dataframe
# Use mutate
# Count how many observations were assigned to each cluster with the count function
cog_df_cl <- mutate(cog_df, cluster = cut_ward)
count(cog_df_cl,cluster)
# Cross-checking clustering results using table function
table(cog_df_cl$cluster,cog_label)
# Add the cluster information from cog_df_cl to clean_data
clean_data$cluster <- cog_df_cl$cluster
# Check the updated structure of clean_data
str(clean_data)
# Checking sex in cluster
table(cog_df_cl$cluster,clean_data$sex)
#--------
cluster_info <- clean_data %>%
select(participant_id, cluster)
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/participants.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
rename(cluster_2 = cluster)
View(cluster_info)
cluster_info <- clean_data %>%
select(participant_id, cluster)
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/participants.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
merged_data <- merged_data %>%
rename(cluster_2 = cluster)
# Check the structure of the merged data to ensure everything is correct
str(merged_data)
# Optionally, save the merged data to a new file
write_tsv(merged_data, "C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data.tsv")
View(cluster_info)
# Hierarchical clustering
library(tidyverse)
library(dplyr)
library(dendextend)
library(ggplot2)
library(gridExtra)
library(purrr)
library(car)
load("clean_data.RData")
# Extract relevant columns from clean_data
cog_df <- clean_data[, c("group","z_pvt_reaction_time_w","z_tmt_a_time_w","z_tmt_b_time_w")]
# Check structure and summarize contents of cog_df
str(cog_df)
summary(cog_df)
# Store group labels in a separate variable and exclude label (group column) from the dataset to do clustering
# Later true labels will be used to check how good clustering turned out
cog_label <- cog_df$group
cog_df$group <- NULL
str(cog_df)
# Build distance matrix
# Since all values are continuous numerical values I use euclidean distance method
dist_mat <- dist(cog_df, method = 'euclidean')
# Now decide which linkage method to use
# Try different kinds of linkage methods after decide which performed better
# Build dendrogram by plotting hierarchical cluster object with hclust
# Specify linkage method via 'method' argument
hclust_ward <- hclust(dist_mat, method = 'ward')
plot(hclust_ward)
# Create the desired number of clusters
# Since I want two groups 'withPCS' and 'withoutPCS' number of clusters = 2
cut_ward <- cutree(hclust_ward, k = 4)
# To visualize clusters on dendrogram use abline function to draw the cut line
plot(hclust_ward)
rect.hclust(hclust_ward, k = 4, border = 2:30)
abline(h = 10, col = 'red')
# Visualize tree with different colored branches
ward_dend_obj <- as.dendrogram(hclust_ward)
ward_col_dend <- color_branches(ward_dend_obj, h = 10)
plot(ward_col_dend)
# Append cluster results obtained back in the original dataframe
# Use mutate
# Count how many observations were assigned to each cluster with the count function
cog_df_cl <- mutate(cog_df, cluster = cut_ward)
count(cog_df_cl,cluster)
# Cross-checking clustering results using table function
table(cog_df_cl$cluster,cog_label)
# Add the cluster information from cog_df_cl to clean_data
clean_data$cluster <- cog_df_cl$cluster
# Check the updated structure of clean_data
str(clean_data)
# Checking sex in cluster
table(cog_df_cl$cluster,clean_data$sex)
#--------
cluster_info <- clean_data %>%
select(participant_id, cluster)
# Load the participants.tsv file
participants <- read.delim("C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data.tsv", na.strings = "n/a", header = TRUE)
# Check the structure of participants to identify a common identifier
str(participants)
# Merge the clean_data with participants based on a common identifier
merged_data <- participants %>%
left_join(cluster_info, by = "participant_id")
merged_data <- merged_data %>%
rename(cluster_4 = cluster)
# Check the structure of the merged data to ensure everything is correct
str(merged_data)
# Optionally, save the merged data to a new file
write_tsv(merged_data, "C:/Users/jankj/OneDrive/Desktop/masters_thesis/data/merged_data_all.tsv")
# 3. demographics
# 4. checking requirements
# 5. have a look at the data (plots)
#------------ 1. load packages------------------
library(tidyverse)
library(carData)
library(car)
library(readr)
library(ggdist)
library(ggExtra)# displaying distributions next to plots
library(ggsignif)# displaying stats in plots
library(ggplot2)
library(ggpubr)
table_power_5 <- read_csv("data/analysis_power/table_power_final_5.csv")
setwd("C:/Users/jankj/OneDrive/Desktop/masters_thesis")
table_power_5 <- read_csv("data/analysis_power/table_power_final_5.csv")
number_of_epochs_5 <- read_csv("data/analysis_power/number_of_epochs_5.csv")
View(number_of_epochs_5)
# this would be the amount of people where more than half of the data is good
number_of_epochs%>%
filter(number_epochs > 37.5)# here 70
number_of_epochs_5%>%
filter(number_epochs > 37.5)# here 62
number_of_epochs_5%>%
filter(number_epochs < 37.5)# these subjects have to be excluded
# modify table (f.ex. add tmt b-a)
table_power_5 <- table_power_5%>%
mutate(facit_f_FS = as.numeric(facit_f_FS),
tmt_b_minus_a = tmt_b_time-tmt_a_time)
table_power_5 <- merge(table_power_5, number_of_epochs_5)
View(table_power_5)
test_table <- table_power_5%>%
group_by(participant_id,group,tmt_a_time,facit_f_FS, tmt_b_minus_a,age,years_of_education,number_epochs,cluster_2)%>%
summarise(mean_delta_power = mean(rel_delta),
mean_beta_power = mean(rel_beta),
mean_aperiodic_exponent = mean(aperiodic_exponent))
test_table%>%
group_by(cluster_2)%>%
summarise(mean_age = mean(age),
sd_age = sd(age),
mean_epochs = mean(number_epochs))# sind immerhin fast identisch vom Alter her
t.test(age~cluster_2, data = test_table, alternative = "two.sided")
# Define the channel names you want to select (for delta)
frontal_channels <- c('22','105','11','40','75','39','49','82','48','19','112','25','94','93','83','92','95','96','21','50','10','59','26')
# Filter rows with the specified channel names
table_power_frontal <- table_power_5%>%
filter(table_power_5$channel %in% frontal_channels)
# Define the channel names you want to select (for beta)
central_channels <- c('85','34','65','37','90','66','1','68','3','67','2','70','74','76','81','34','37','42','86','43','87','44','88','45','89','46','77','5','78','6','7','79','8','80','71','35','72','36','73')
# Filter rows with the specified channel names
table_power_central <- table_power_5%>%
filter(table_power_5$channel %in% central_channels)
#-------3. summarise mean -----------------
df_corr_frontal <- table_power_frontal%>%
group_by(participant_id,group,tmt_a_time,facit_f_FS, tmt_b_minus_a,age,years_of_education,sex,hads_d_total_score, number_epochs, cluster_2)%>%
summarise(mean_delta_power = mean(rel_delta),
mean_beta_power = mean(rel_beta),
mean_aperiodic_exponent = mean(aperiodic_exponent))
df_corr_central <- table_power_central%>%
group_by(participant_id,group,tmt_a_time,facit_f_FS, tmt_b_minus_a,hads_d_total_score, number_epochs, cluster_2)%>%
summarise(mean_delta_power = mean(rel_delta),
mean_beta_power = mean(rel_beta),
mean_aperiodic_exponent = mean(aperiodic_exponent))
View(df_corr_central)
View(df_corr_frontal)
View(number_of_epochs_5)
View(table_power_5)
View(table_power_central)
#------- 4. demographics-----------------
# age
df_corr_frontal%>%
group_by(participant_id)%>%
ggplot(aes(age))+
geom_histogram(color = "black",
fill = "white", bins = sqrt(100))+
facet_wrap(~cluster_2,scales = 'free')+
theme_classic()
# years of education
df_corr_frontal%>%
group_by(participant_id)%>%
ggplot(aes(years_of_education))+
geom_histogram(color = "black",
fill = "white", bins = sqrt(100))+
facet_wrap(~cluster_2,scales = 'free')+
theme_classic()
t.test(years_of_education~cluster_2, data = df_corr_frontal, alternative = "two.sided")
wilcox.test(years_of_education~cluster_2, data = df_corr_frontal,
exact = FALSE,
correct = FALSE,
conf.int = FALSE)# without PCS hat eine schiefe Verteilung
df_corr_frontal%>%
group_by(cluster_2)%>%
count()
df_corr_frontal%>%
group_by(cluster_2,sex)%>%
count()
df_corr_frontal%>%
group_by(cluster_2)%>%
summarise(mean_age = mean(age),
sd_age = sd(age))
t.test(age~cluster_2, data = df_corr_frontal, alternative = "two.sided")
wilcox.test(age~cluster_2, data = df_corr_frontal,
exact = FALSE,
correct = FALSE,
conf.int = FALSE)# 0.69
table_behav <- df_corr_frontal%>%
group_by(cluster_2)%>%
summarise(mean_facit = mean(facit_f_FS, na.rm = T),
sd_facit = sd(facit_f_FS, na.rm = T),
mean_hads = mean(hads_d_total_score, na.rm = T),
sd_hads = sd(hads_d_total_score, na.rm = T),
mean_tmta = mean(tmt_a_time),
sd_tmta = sd(tmt_a_time),
mean_tmtb_a = mean(tmt_b_minus_a),
sd_tmtb_a = sd(tmt_b_minus_a),
mean_y_o = mean(years_of_education),
sd_y_o = sd(years_of_education),
mean_epoc = mean(number_epochs),
sd_epoc = sd(number_epochs))
df_corr_frontal%>%
group_by(cluster_2)%>%
ggplot(aes(facit_f_FS))+
geom_histogram(color = "black",
fill = "white", bins = sqrt(100))+
facet_wrap(~cluster_2,scales = 'free')+
theme_classic()# eine Gruppe ist schief!
leveneTest(facit_f_FS~cluster_2,data = df_corr_frontal)# nicht significant
# wilcox.test
wilcox.test(facit_f_FS~cluster_2, data = df_corr_frontal,
exact = FALSE,
correct = FALSE,
conf.int = FALSE)# 0.002
t.test(facit_f_FS~cluster_2, data = df_corr_frontal, alternative = "two.sided", paired = FALSE)# significant p = 0.001083
df_corr_frontal%>%
group_by(cluster_2)%>%
ggplot(aes(hads_d_total_score))+
geom_histogram(color = "black",
fill = "white", bins = sqrt(100))+
facet_wrap(~cluster_2,scales = 'free')+
theme_classic()# eine Gruppe ist schief
leveneTest(hads_d_total_score~cluster_2,data = df_corr_frontal)# not significant
t.test(hads_d_total_score~cluster_2, data = df_corr_frontal, alternative = "two.sided", paired = FALSE)# significant p = 0.001083
wilcox.test(hads_d_total_score~cluster_2, data = df_corr_frontal,
exact = FALSE,
correct = FALSE,
conf.int = FALSE)# without PCS hat eine schiefe Verteilung
df_corr_frontal%>%
group_by(cluster_2)%>%
ggplot(aes(tmt_a_time))+
geom_histogram(color = "black",
fill = "white", bins = sqrt(100))+
facet_wrap(~cluster_2,scales = 'free')+
theme_classic()# sieht okay aus
leveneTest(tmt_a_time~cluster_2,data = df_corr_frontal)# not significant
wilcox.test(tmt_a_time~cluster_2, data = df_corr_frontal,
exact = FALSE,
correct = FALSE,
conf.int = FALSE)
df_corr_frontal%>%
group_by(cluster_2)%>%
ggplot(aes(tmt_b_minus_a))+
geom_histogram(color = "black",
fill = "white", bins = sqrt(100))+
facet_wrap(~cluster_2,scales = 'free')+
theme_classic()# beide leicht schief (gleich)
leveneTest(tmt_b_minus_a~cluster_2,data = df_corr_frontal)# not significant
t.test(tmt_b_minus_a~cluster_2, data = df_corr_frontal, alternative = "two.sided", paired = FALSE)# significant p = 0.001083
wilcox.test(tmt_b_minus_a~cluster_2, data = df_corr_frontal,
exact = FALSE,
correct = FALSE,
conf.int = FALSE)# without PCS hat eine schiefe Verteilung
# number of epochs
df_corr_frontal%>%
group_by(cluster_2)%>%
summarise(mean_epoch = mean(number_epochs),
sd_age = sd(number_epochs))
t.test(number_epochs~cluster_2, data = df_corr_frontal, alternative = "two.sided", paired = FALSE)
df_corr_frontal%>%
ggplot(aes(x = number_epochs, y = mean_delta_power))+
geom_point()
cor.test(df_corr_frontal$mean_delta_power,df_corr_frontal$number_epochs)
df_corr_central%>%
ggplot(aes(x = number_epochs, y = mean_beta_power))+
geom_point()
cor.test(df_corr_central$mean_delta_power,df_corr_central$number_epochs)
# have a look at the whole data set (boxplots)
table_power_5%>%
group_by(cluster_2)%>%
ggplot(aes(x = group, y = rel_delta, color = group))+
geom_boxplot()
# have a look at the whole data set (boxplots)
table_power_5%>%
group_by(cluster_2)%>%
ggplot(aes(x = cluster_2, y = rel_delta, color = cluster_2))+
geom_boxplot()
# per group
table_power_5%>%
group_by(cluster_2)%>%
ggplot(aes(x = cluster_2, y = rel_beta, color = cluster_2))+
geom_boxplot()
# just the frontal channels
table_power_frontal%>%
group_by(participant_id, cluster_2)%>%
ggplot(aes(x = participant_id, y = rel_delta, color = group))+
geom_boxplot()#+
# just per group
df_corr_frontal%>%
group_by(cluster_2)%>%
ggplot(aes(x = group, y = mean_delta_power, color = group))+
geom_boxplot()
# just per group (beta)
df_corr_central%>%
group_by(cluster_2)%>%
ggplot(aes(x = group, y = mean_beta_power, color = group))+
geom_boxplot()
# just per group (beta)
df_corr_central%>%
group_by(cluster_2)%>%
ggplot(aes(x = cluster_2, y = mean_beta_power, color = group))+
geom_boxplot()
# just per group (beta)
df_corr_central%>%
group_by(cluster_2)%>%
ggplot(aes(x = cluster_2, y = mean_beta_power, color = cluster_2))+
geom_boxplot()
